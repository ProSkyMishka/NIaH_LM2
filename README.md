# NIaH_LM2
# Отчёт по оценке Needle-in-a-Haystack (NIaH)

В данном отчёте представлены результаты эксперимента Needle-in-a-Haystack (NIaH), проведённого для архитектуры LM2. Методология NIaH используется для оценки способности модели извлекать конкретную целевую информацию («иголку»), помещённую на различной глубине внутри длинного входного контекста.

## 1. Описание методологии

Эксперимент NIaH заключается во внедрении заранее известного предложения (*needle*) в большой текстовый контекст (*haystack*) и проверке того, способна ли модель корректно воспроизвести или распознать это предложение по запросу. Ключевым параметром является относительная глубина расположения *needle*, выраженная в процентах от общей длины контекста.

## 2. Экспериментальная постановка

В качестве *needle* использовалось следующее утверждение:

> «The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.»

Данное предложение встраивалось в контексты длиной 2048, 4096 и 8192 токенов. Глубина расположения *needle* варьировалась от 1% до 99% длины контекста. Для каждой комбинации параметров проводился отдельный прогон модели.

## 3. Метрики оценки

Основной метрикой качества являлась бинарная метрика `score`: значение **1** присваивалось в случае корректного извлечения *needle* моделью, и **0** — в противном случае. Дополнительно измерялось время выполнения теста (`test_duration_seconds`), характеризующее вычислительную стоимость инференса.

## 4. Результаты эксперимента

| Длина контекста | Средняя точность | Наблюдения |
| :-------------- | :--------------- | :--------- |
| 2048 токенов    | 1.00             | Идеальное качество на всех глубинах расположения *needle*. |
| 4096 токенов    | 0.44             | Успешное извлечение преимущественно при глубине ≥ 75%. |
| 8192 токенов    | 0.33             | Стабильное извлечение наблюдается только при глубине ≥ 90%. |

## 5. Анализ кривых NIaH

Кривые NIaH, представляющие зависимость точности от глубины расположения *needle*, наглядно демонстрируют выраженный эффект **«recency bias»**. По мере роста длины контекста способность модели удерживать и извлекать информацию, расположенную в начале или середине текста, резко снижается. Фактически, для длинных контекстов модель опирается преимущественно на последние сегменты входа.

## 6. Анализ производительности

Анализ времени выполнения показал почти линейный рост задержки инференса с увеличением длины контекста.

| Длина контекста | Среднее время выполнения (сек.) |
| :-------------- | :------------------------------ |
| 2048 токенов    | 0.69                            |
| 4096 токенов    | 1.35                            |
| 8192 токенов    | 2.72                            |

Это указывает на ожидаемую вычислительную сложность обработки длинных последовательностей.

## 7. Обсуждение результатов

Полученные результаты свидетельствуют о том, что архитектура LM2 эффективно работает с контекстами умеренной длины, однако сталкивается с серьёзными ограничениями при попытке извлечения информации из ранних частей длинного контекста. Наблюдаемое поведение согласуется с известными ограничениями трансформерных архитектур, у которых эффективное использование контекста зачастую значительно меньше номинального окна.

## 8. Заключение

В рамках проведённого эксперимента NIaH было показано, что архитектура `LM2` обеспечивает надёжное извлечение информации при длине контекста **2048 токенов**, но демонстрирует выраженное ухудшение качества при увеличении контекста до **4096** и **8192 токенов**. Результаты подчёркивают необходимость использования архитектурных улучшений, таких как внешняя память или специализированные механизмы long-context reasoning, для устойчивой работы с длинными текстами.
